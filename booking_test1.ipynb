{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04244a20-f3cd-4ca4-93df-daa737866338",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /opt/conda/lib/python3.9/site-packages (2.6.1)\n",
      "Requirement already satisfied: lxml>=3.5.0 in /opt/conda/lib/python3.9/site-packages (from scrapy) (4.8.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /opt/conda/lib/python3.9/site-packages (from scrapy) (36.0.1)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /opt/conda/lib/python3.9/site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /opt/conda/lib/python3.9/site-packages (from scrapy) (0.6.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from scrapy) (59.8.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /opt/conda/lib/python3.9/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /opt/conda/lib/python3.9/site-packages (from scrapy) (22.0.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /opt/conda/lib/python3.9/site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: tldextract in /opt/conda/lib/python3.9/site-packages (from scrapy) (3.3.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /opt/conda/lib/python3.9/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /opt/conda/lib/python3.9/site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in /opt/conda/lib/python3.9/site-packages (from scrapy) (21.1.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in /opt/conda/lib/python3.9/site-packages (from scrapy) (22.4.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in /opt/conda/lib/python3.9/site-packages (from scrapy) (0.2.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.9/site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /opt/conda/lib/python3.9/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.0)\n",
      "Requirement already satisfied: six>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from parsel>=1.5.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.9/site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.9/site-packages (from service-identity>=16.0.0->scrapy) (21.4.0)\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.9/site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.9/site-packages (from Twisted>=17.9.0->scrapy) (4.0.1)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /opt/conda/lib/python3.9/site-packages (from Twisted>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in /opt/conda/lib/python3.9/site-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in /opt/conda/lib/python3.9/site-packages (from Twisted>=17.9.0->scrapy) (21.3.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /opt/conda/lib/python3.9/site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from tldextract->scrapy) (3.7.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/conda/lib/python3.9/site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.9/site-packages (from tldextract->scrapy) (3.3)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.9/site-packages (from tldextract->scrapy) (2.27.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.21)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.1.0->tldextract->scrapy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec812db-411c-4580-a91c-c52a2ba9f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae36b505-7cb7-404b-9444-085f935222ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"Mont Saint Michel\",\n",
    "\"St Malo\",\n",
    "\"Bayeux\",\n",
    "\"Le Havre\",\n",
    "\"Rouen\",\n",
    "\"Paris\",\n",
    "\"Amiens\",\n",
    "\"Lille\",\n",
    "\"Strasbourg\",\n",
    "\"Chateau du Haut Koenigsbourg\",\n",
    "\"Colmar\",\n",
    "\"Eguisheim\",\n",
    "\"Besancon\",\n",
    "\"Dijon\",\n",
    "\"Annecy\",\n",
    "\"Grenoble\",\n",
    "\"Lyon\",\n",
    "\"Gorges du Verdon\",\n",
    "\"Bormes les Mimosas\",\n",
    "\"Cassis\",\n",
    "\"Marseille\",\n",
    "\"Aix en Provence\",\n",
    "\"Avignon\",\n",
    "\"Uzes\",\n",
    "\"Nimes\",\n",
    "\"Aigues Mortes\",\n",
    "\"Saintes Maries de la mer\",\n",
    "\"Collioure\",\n",
    "\"Carcassonne\",\n",
    "\"Ariege\",\n",
    "\"Toulouse\",\n",
    "\"Montauban\",\n",
    "\"Biarritz\",\n",
    "\"Bayonne\",\n",
    "\"La Rochelle\"] #la liste des villes dont nous allons chercher les informations\n",
    "\n",
    "cities_for_dic = [x.replace(\" \",\"_\") for x in cities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ec6f93-162b-4d31-a871-e187261f8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookingSpider(scrapy.Spider):\n",
    "    name = \"booking\"\n",
    "# Starting URL\n",
    "    start_urls = ['https://www.booking.com/']\n",
    "\n",
    "    # Parse function for form request\n",
    "    def parse(self, response):\n",
    "        for city in cities_for_dic:\n",
    "\n",
    "            yield scrapy.FormRequest.from_response(\n",
    "                response,\n",
    "                formdata={ \"ss\": city, \n",
    "                          \"data-checkin\":\"2022-06-06\",\n",
    "                          \"data-checkout\":\"2022-06-10\",\n",
    "\n",
    "                          },\n",
    "                callback=self.after_search\n",
    "            )\n",
    "\n",
    "    # Callback used after search\n",
    "    def after_search(self, response):\n",
    "\n",
    "\n",
    "        results = response.css('div.a1b3f50dcd.f7c6687c3d.a1f3ecff04.f996d8c258')\n",
    "\n",
    "        for r in results:\n",
    "            yield {\n",
    "                'hotel_name': r.css('h3.a4225678b2 ::text').get(),\n",
    "                'url' : r.css('h3.a4225678b2 ::attr(href)').get(),\n",
    "                'city': r.css('span.f4bd0794db.b4273d69aa ::text').get(),\n",
    "                'Score given by the website users': r.css('div.b5cd09854e.d10a6220b4::text').get(),\n",
    "                'Text description of the hotel': r.css('div.d8eab2cf7f ::text').get(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa8f78db-0f18-4bf8-bfc1-9a6f93e2e1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 13:03:55 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-31 13:03:55 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 36.0.1, Platform Linux-5.4.170+-x86_64-with-glibc2.31\n",
      "2022-05-31 13:03:55 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2022-05-31 13:03:55 [scrapy.extensions.telnet] INFO: Telnet Password: eb1cbc5f04e9c511\n",
      "2022-05-31 13:03:55 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-31 13:03:55 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-31 13:03:55 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-31 13:03:55 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-31 13:03:55 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-31 13:03:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-31 13:03:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-31 13:03:59 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-31 13:03:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 19879,\n",
      " 'downloader/request_count': 21,\n",
      " 'downloader/request_method_count/GET': 21,\n",
      " 'downloader/response_bytes': 33172,\n",
      " 'downloader/response_count': 21,\n",
      " 'downloader/response_status_count/301': 5,\n",
      " 'downloader/response_status_count/302': 16,\n",
      " 'elapsed_time_seconds': 3.807844,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 31, 13, 3, 59, 818367),\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 101740544,\n",
      " 'memusage/startup': 101740544,\n",
      " 'scheduler/dequeued': 21,\n",
      " 'scheduler/dequeued/memory': 21,\n",
      " 'scheduler/enqueued': 21,\n",
      " 'scheduler/enqueued/memory': 21,\n",
      " 'start_time': datetime.datetime(2022, 5, 31, 13, 3, 56, 10523)}\n",
      "2022-05-31 13:03:59 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"booking.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('results/'):\n",
    "        os.remove('results/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'results/' + filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(BookingSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29877973-ee08-49d0-867e-57503bec6a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
